#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ˜å ±åŠ æ‹¿å¤§æ¸¯è (HK-GA) Wayback Machine å­˜æª”å·¥å…·

åŠŸèƒ½ï¼š
- å¾æŒ‡å®šæ—¥æœŸç¯„åœæå–æ‰€æœ‰ HK-GA æ–‡ç« 
- å­˜æª”è‡³ Wayback Machine (web.archive.org)
- è‡ªå‹•è¨˜éŒ„é€²åº¦èˆ‡å¤±æ•—é …ç›®
- éµå®ˆ rate limiting å’ŒéŒ¯èª¤é‡è©¦æ©Ÿåˆ¶
- å¯é¸ä½¿ç”¨ newspaper3k è‡ªå‹•ç™¼ç¾æ–‡ç«  URL
- æ”¯æ´ç¹é«”ä¸­æ–‡é—œéµè©éæ¿¾

ä½¿ç”¨æ–¹æ³•ï¼š
    python run_archiver.py
"""

import requests
import time
import json
import sqlite3
import unicodedata
import re
from datetime import datetime, timedelta
from pathlib import Path
import logging
import sys
import argparse
from typing import List, Dict, Tuple, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


class RateLimiter:
    """Pre-request rate limiting with token bucket algorithm"""

    def __init__(self, delay: float, max_burst: int = 3):
        self.delay = delay
        self.tokens = max_burst
        self.max_tokens = max_burst
        self.last_request = time.time()
        self.lock = threading.Lock()

    def acquire(self):
        """Wait if needed before making request"""
        with self.lock:
            now = time.time()
            elapsed = now - self.last_request
            self.tokens = min(self.max_tokens, self.tokens + elapsed / self.delay)
            if self.tokens < 1:
                wait_time = (1 - self.tokens) * self.delay
                time.sleep(wait_time)
                self.tokens = 0
            else:
                self.tokens -= 1
            self.last_request = time.time()


class MingPaoHKGAArchiver:
    """æ˜å ±æ¸¯èå­˜æª”ä¸»è¦é¡åˆ¥"""

    WAYBACK_SAVE_URL = "https://web.archive.org/save/{url}"
    BASE_URL = "http://www.mingpaocanada.com/tor"

    HK_GA_PREFIXES = [
        "gaa",
        "gab",
        "gac",
        "gad",
        "gae",
        "gaf",
        "gba",
        "gbb",
        "gbc",
        "gbd",
        "gbe",
        "gbf",
        "gca",
        "gcb",
        "gcc",
        "gcd",
        "gce",
        "gcf",
        "gga",
        "ggb",
        "ggc",
        "ggd",
        "gge",
        "ggf",
        "ggh",
        "gha",
        "ghb",
        "ghc",
        "ghd",
        "gma",
        "gmb",
    ]

    def __init__(self, config_path="config.json"):
        """åˆå§‹åŒ–å­˜æª”å™¨"""
        self.config = self.load_config(config_path)
        self.setup_logging()
        self.setup_database()
        self.setup_directories()

        self.stats = {
            "total_attempted": 0,
            "successful": 0,
            "failed": 0,
            "already_archived": 0,
            "rate_limited": 0,
            "not_found": 0,
        }
        self.stats_lock = threading.Lock()

        rate_limit_delay = self.config["archiving"]["rate_limit_delay"]
        self.rate_limiter = RateLimiter(delay=rate_limit_delay, max_burst=1)

        self.logger.info("=" * 60)
        self.logger.info("æ˜å ±åŠ æ‹¿å¤§æ¸¯è (HK-GA) Wayback Machine å­˜æª”å·¥å…·")
        self.logger.info(f"å•Ÿå‹•æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 60)

    def _make_request(self, method: str, url: str, **kwargs) -> requests.Response:
        """Rate-limited HTTP request wrapper

        Ensures all outbound HTTP requests (GET, HEAD, POST) respect the global
        rate limiter to prevent connection resets from the server.

        Args:
            method: HTTP method ('GET', 'POST', 'HEAD')
            url: Target URL
            **kwargs: Additional arguments passed to requests

        Returns:
            requests.Response object

        Raises:
            ValueError: If unsupported HTTP method specified
        """
        self.rate_limiter.acquire()

        method_upper = method.upper()
        if method_upper == "GET":
            return requests.get(url, **kwargs)
        elif method_upper == "POST":
            return requests.post(url, **kwargs)
        elif method_upper == "HEAD":
            return requests.head(url, **kwargs)
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")

    def load_config(self, config_path):
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        default_config = {
            "database": {"path": "hkga_archive.db"},
            "logging": {"level": "INFO", "file": "logs/hkga_archiver.log"},
            "archiving": {
                "rate_limit_delay": 3,
                "verify_first": True,
                "timeout": 30,
                "max_retries": 3,
                "retry_delay": 10,
            },
            "daily_limit": 2000,
            "date_range": {"start": "2025-01-01", "end": "2025-01-31"},
            "use_newspaper": False,
            "parallel": {
                "enabled": True,
                "max_workers": 3,
                "rate_limit_delay": 1.0,
            },
            "keywords": {
                "enabled": False,
                "terms": ["é¦™æ¸¯", "æ”¿æ²»", "ä¸­åœ‹", "å°ç£", "åœ‹å®‰æ³•", "é¸èˆ‰", "ç¤ºå¨"],
                "case_sensitive": False,
                "language": "zh-TW",
                "script": "traditional",
                "normalization": "NFC",
                "logic": "or",
                "search_content": False,
                "parallel_workers": 2,
                "wayback_first": True,
            },
        }

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                user_config = json.load(f)
                self.merge_config(default_config, user_config)
                return default_config
        except FileNotFoundError:
            self.logger = logging.getLogger(__name__)
            self.logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜èªé…ç½®")
            return default_config

    def merge_config(self, default, user):
        """åˆä½µé…ç½®"""
        for key, value in user.items():
            if key in default and isinstance(default[key], dict):
                self.merge_config(default[key], value)
            else:
                default[key] = value

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        log_config = self.config["logging"]
        log_level = getattr(logging, log_config["level"].upper())

        Path("logs").mkdir(exist_ok=True)

        logging.basicConfig(
            level=log_level,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(log_config["file"]),
                logging.StreamHandler(sys.stdout),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def setup_database(self):
        """è¨­ç½® SQLite æ•¸æ“šåº«"""
        db_path = self.config["database"]["path"]
        self.conn = sqlite3.connect(db_path)
        self.conn.execute("PRAGMA encoding = 'UTF-8'")
        self.cursor = self.conn.cursor()

        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS archive_records (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_url TEXT UNIQUE,
                wayback_url TEXT,
                archive_date TEXT,
                status TEXT,
                http_status INTEGER,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                matched_keywords TEXT,
                checked_wayback BOOLEAN DEFAULT FALSE,
                title_search_only BOOLEAN DEFAULT FALSE,
                article_title TEXT
            )
        """)

        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS daily_progress (
                date TEXT PRIMARY KEY,
                articles_found INTEGER,
                articles_archived INTEGER,
                articles_failed INTEGER,
                articles_not_found INTEGER,
                execution_time REAL,
                completed_at TIMESTAMP,
                keywords_filtered INTEGER DEFAULT 0
            )
        """)

        self.cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_status ON archive_records(status);"
        )
        self.cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_date ON archive_records(archive_date);"
        )
        self.cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_keywords ON archive_records(matched_keywords);"
        )
        # Performance optimization: Speed up duplicate URL checks
        self.cursor.execute(
            "CREATE UNIQUE INDEX IF NOT EXISTS idx_article_url ON archive_records(article_url);"
        )
        # Compound index for common query pattern (checking if URL exists with status)
        self.cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_url_status ON archive_records(article_url, status);"
        )

        try:
            self.cursor.execute(
                "ALTER TABLE archive_records ADD COLUMN matched_keywords TEXT"
            )
        except sqlite3.OperationalError:
            pass
        try:
            self.cursor.execute(
                "ALTER TABLE archive_records ADD COLUMN checked_wayback INTEGER DEFAULT 0"
            )
        except sqlite3.OperationalError:
            pass
        try:
            self.cursor.execute(
                "ALTER TABLE archive_records ADD COLUMN title_search_only INTEGER DEFAULT 0"
            )
        except sqlite3.OperationalError:
            pass
        try:
            self.cursor.execute(
                "ALTER TABLE archive_records ADD COLUMN article_title TEXT"
            )
        except sqlite3.OperationalError:
            pass
        try:
            self.cursor.execute(
                "ALTER TABLE daily_progress ADD COLUMN keywords_filtered INTEGER DEFAULT 0"
            )
        except sqlite3.OperationalError:
            pass

        self.conn.commit()
        self.logger.info(f"æ•¸æ“šåº«å·²åˆå§‹åŒ–: {db_path}")

    def setup_directories(self):
        """å‰µå»ºå¿…è¦çš„ç›®éŒ„"""
        Path("output").mkdir(exist_ok=True)
        Path("logs").mkdir(exist_ok=True)

    def generate_article_urls(self, target_date: datetime) -> List[str]:
        """ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰å¯èƒ½ HK-GA æ–‡ç«  URL"""
        if self.config.get("use_newspaper", False):
            return self.generate_article_urls_newspaper(target_date)

        # Use index-based discovery if enabled (much more efficient)
        if self.config.get("use_index_page", True):
            return self._generate_urls_from_index(target_date)

        return self._generate_urls_bruteforce(target_date)

    def _generate_urls_from_index(self, target_date: datetime) -> List[str]:
        """å¾ç´¢å¼•é çˆ¬å–å¯¦éš›å­˜åœ¨çš„æ–‡ç«  URL (æ¨è–¦æ–¹æ³•)

        æ­¤æ–¹æ³•æ¯”æš´åŠ›ç”Ÿæˆæ›´é«˜æ•ˆï¼š
        - åªè¿”å›çœŸå¯¦å­˜åœ¨çš„æ–‡ç«  (~30-40 ç¯‡/å¤©)
        - ç„¡éœ€é©—è­‰ URL æ˜¯å¦å­˜åœ¨
        - ä¸æœƒç”¢ç”Ÿ 404 éŒ¯èª¤
        - è‡ªå‹•ç™¼ç¾æ–°çš„ URL æ¨¡å¼
        """
        date_str = target_date.strftime("%Y%m%d")
        index_url = f"{self.BASE_URL}/htm/News/{date_str}/HK-GAindex_r.htm"

        try:
            self.logger.debug(f"å¾ç´¢å¼•é çˆ¬å–: {index_url}")
            response = requests.get(
                index_url,
                timeout=self.config["archiving"]["timeout"],
                headers={
                    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
                },
            )

            if response.status_code != 200:
                self.logger.warning(
                    f"ç´¢å¼•é ä¸å­˜åœ¨ ({response.status_code})ï¼Œå›é€€åˆ°æš´åŠ›æ¨¡å¼: {date_str}"
                )
                return self._generate_urls_bruteforce(target_date)

            # Parse HTML to extract article URLs
            import re

            article_urls = set()

            # Find all href attributes in listing sections
            # Pattern matches: href="../../../htm/News/YYYYMMDD/HK-xxxN_r.htm"
            pattern = r'href="([^"]*htm/News/\d{8}/HK-[^"]+_r\.htm)"'
            matches = re.findall(pattern, response.text)

            for relative_path in matches:
                # Skip index pages (we only want actual articles)
                if "index" in relative_path.lower():
                    continue

                # Convert relative path to absolute URL
                # ../../../htm/News/20260113/HK-gaa1_r.htm -> http://www.mingpaocanada.com/tor/htm/News/20260113/HK-gaa1_r.htm
                absolute_url = relative_path.replace("../../../", f"{self.BASE_URL}/")
                article_urls.add(absolute_url)

            article_list = sorted(list(article_urls))
            self.logger.info(
                f"å¾ç´¢å¼•é ç™¼ç¾ {len(article_list)} ç¯‡æ–‡ç«  (æ—¥æœŸ: {date_str})"
            )
            return article_list

        except requests.exceptions.RequestException as e:
            self.logger.warning(f"ç´¢å¼•é çˆ¬å–å¤±æ•—: {str(e)}ï¼Œå›é€€åˆ°æš´åŠ›æ¨¡å¼")
            return self._generate_urls_bruteforce(target_date)

    def _generate_urls_bruteforce(self, target_date: datetime) -> List[str]:
        """æš´åŠ›ç”Ÿæˆ URL (å‚™ç”¨æ–¹æ³•)

        æ³¨æ„ï¼šæ­¤æ–¹æ³•æœƒç”Ÿæˆå¤§é‡ä¸å­˜åœ¨çš„ URL (~1,120 å€‹/å¤©)
        å»ºè­°ä½¿ç”¨ use_index_page: true æ”¹ç”¨ç´¢å¼•é çˆ¬å–
        """
        date_str = target_date.strftime("%Y%m%d")
        base_path = f"{self.BASE_URL}/htm/News/{date_str}"

        article_urls = []

        for prefix in self.HK_GA_PREFIXES:
            for num in range(1, 9):
                url = f"{base_path}/HK-{prefix}{num}_r.htm"
                article_urls.append(url)

        self.logger.debug(f"æš´åŠ›ç”Ÿæˆ {len(article_urls)} å€‹å¯èƒ½ URL çµ¦æ—¥æœŸ {date_str}")
        return article_urls

    def generate_article_urls_newspaper(self, target_date: datetime) -> List[str]:
        """
        ä½¿ç”¨ newspaper4k ç™¼ç¾æ–‡ç«  URL (å·²æ£„ç”¨)

        è­¦å‘Š: æ­¤æ–¹æ³•ä¸é©ç”¨æ–¼æ˜å ±ç¶²ç«™ï¼Œå› ç‚ºç¶²ç«™çµæ§‹ä¸ç¬¦åˆ newspaper4k çš„è§£ææ¨¡å¼ã€‚
        å»ºè­°ä½¿ç”¨é è¨­çš„æš´åŠ› URL ç”Ÿæˆæ¨¡å¼ã€‚
        """
        self.logger.warning(
            "newspaper4k URL ç™¼ç¾æ¨¡å¼ä¸é©ç”¨æ–¼æ˜å ±ç¶²ç«™ï¼Œå°‡ä½¿ç”¨æš´åŠ›æ¨¡å¼ç”Ÿæˆ URL"
        )
        self.logger.warning(
            "å»ºè­°ç§»é™¤ --newspaper åƒæ•¸æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­è¨­ç½® 'use_newspaper': false"
        )
        return self._generate_urls_bruteforce(target_date)

    def discover_articles_full(self, base_url: Optional[str] = None) -> List[Dict]:
        """
        ä½¿ç”¨ newspaper4k ç™¼ç¾ä¸¦æå–æ–‡ç« å®Œæ•´ä¿¡æ¯ (å·²æ£„ç”¨)

        è­¦å‘Š: æ­¤æ–¹æ³•ä¸é©ç”¨æ–¼æ˜å ±ç¶²ç«™ã€‚å»ºè­°ä½¿ç”¨æš´åŠ› URL ç”Ÿæˆ + newspaper4k å…§å®¹æå–ã€‚
        """
        self.logger.warning("discover_articles_full ä¸é©ç”¨æ–¼æ˜å ±ç¶²ç«™ï¼Œå·²æ£„ç”¨")
        return []

    def normalize_cjkv_text(self, text: str) -> str:
        """Normalize CJKV text for consistent matching"""
        if not text:
            return ""
        text = re.sub(r"\s+", " ", text)
        try:
            # Normalize to NFC form
            return unicodedata.normalize("NFC", text)
        except Exception:
            return text

    def check_cjkv_keywords(
        self, text: str, terms: List[str], case_sensitive: bool = False
    ) -> List[str]:
        """Check CJKV keywords in text using OR logic"""
        if not text or not terms:
            return []

        text_normalized = self.normalize_cjkv_text(text)
        text_search = text_normalized if case_sensitive else text_normalized.lower()

        matched = []
        for term in terms:
            term_normalized = self.normalize_cjkv_text(term)
            term_search = term_normalized if case_sensitive else term_normalized.lower()

            if term_search in text_search:
                matched.append(term)

        return matched

    def check_wayback_exists(
        self, url: str, timeout: int = 10
    ) -> Tuple[bool, Optional[str]]:
        """Check if URL exists in Wayback Machine, return (exists, wayback_url)"""
        wayback_url = f"https://web.archive.org/web/2/{url}"
        try:
            response = self._make_request("GET", wayback_url, timeout=timeout)
            if response.status_code == 200:
                return True, wayback_url
        except Exception as e:
            self.logger.debug(f"Wayback check failed: {url[:50]} - {str(e)}")
        return False, None

    def extract_title_from_html(self, html: str) -> str:
        """Extract title from HTML content with proper Traditional Chinese encoding"""
        if not html:
            return ""

        try:
            import re

            # Try og:title first (more reliable)
            og_title_match = re.search(
                r'<meta\s+property="og:title"\s+content="([^"]+)"',
                html,
                re.IGNORECASE,
            )
            if og_title_match:
                title = og_title_match.group(1)
                return self.normalize_cjkv_text(title.strip())

            # Fallback to <title> tag
            title_match = re.search(r"<title[^>]*>([^<]+)</title>", html, re.IGNORECASE)
            if title_match:
                title = title_match.group(1)
                title = re.sub(r"\s+", " ", title).strip()

                # Try different encodings for Traditional Chinese
                encodings_to_try = ["big5-hkscs", "big5", "utf-8"]

                for enc in encodings_to_try:
                    try:
                        # First try to decode as the original encoding was likely mis-detected
                        decoded = title.encode("ISO-8859-1").decode(enc)
                        # Normalize the result
                        normalized = self.normalize_cjkv_text(decoded)
                        if normalized and any(
                            "\u4e00" <= c <= "\u9fff" for c in normalized
                        ):
                            return normalized
                    except (UnicodeDecodeError, LookupError):
                        continue

                # Fallback to UTF-8
                return self.normalize_cjkv_text(title)

        except Exception as e:
            self.logger.debug(f"Title extraction failed: {str(e)}")

        return ""

    def extract_title_with_newspaper4k(self, url: str) -> str:
        """
        Extract title using newspaper4k (more robust for some sites)

        This is an alternative to HTML parsing that may work better
        for sites with complex encodings or structures.
        """
        try:
            from newspaper_extractor import extract_title_only

            title = extract_title_only(url, language="zh", timeout=10)
            if title:
                return self.normalize_cjkv_text(title)
        except ImportError:
            self.logger.debug("newspaper_extractor not available")
        except Exception as e:
            self.logger.debug(f"newspaper4k title extraction failed: {str(e)}")

        return ""

    def fetch_html_content(self, url: str, timeout: int = 15) -> Tuple[str, bool]:
        """Fetch HTML content from URL, return (html, from_wayback)"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
            }

            wayback_first = self.config.get("keywords", {}).get("wayback_first", True)

            # Check Wayback first with longer timeout
            if wayback_first:
                wayback_url = f"https://web.archive.org/web/2/{url}"
                try:
                    response = self._make_request(
                        "GET", wayback_url, timeout=timeout * 2, headers=headers
                    )
                    if response.status_code == 200 and response.text.strip():
                        return response.text, True
                except Exception as e:
                    self.logger.debug(f"Wayback check failed: {url[:50]} - {str(e)}")

            # Try original site with shorter timeout and retry
            for attempt in range(2):
                try:
                    response = self._make_request(
                        "GET",
                        url,
                        timeout=timeout / 2 if attempt == 0 else timeout,
                        headers=headers,
                    )
                    if response.status_code == 200 and response.text.strip():
                        return response.text, False
                except requests.exceptions.ConnectionError as e:
                    if "Connection reset by peer" in str(e) and attempt == 0:
                        self.logger.debug(
                            f"Connection reset on first attempt for {url[:50]}, retrying..."
                        )
                        time.sleep(1)
                        continue
                    raise e
                except Exception as e:
                    self.logger.debug(f"Fetch failed: {url[:50]} - {str(e)}")
                    raise e

        except Exception as e:
            self.logger.debug(f"Fetch failed: {url[:50]} - {str(e)}")

        return "", False

    def filter_urls_by_keywords(
        self, urls: List[str], parallel: bool = False
    ) -> List[Dict]:
        """Filter URLs by keywords with optional parallel processing"""
        keywords_config = self.config.get("keywords", {})
        if not keywords_config.get("enabled", False):
            return [{"url": url, "should_archive": True} for url in urls]

        terms = keywords_config.get("terms", [])
        search_content = keywords_config.get("search_content", False)
        case_sensitive = keywords_config.get("case_sensitive", False)

        if not terms:
            self.logger.warning("é—œéµè©åˆ—è¡¨ç‚ºç©ºï¼Œè·³ééæ¿¾")
            return [{"url": url, "should_archive": True} for url in urls]

        if parallel and not search_content:
            return self._filter_keywords_parallel(urls, terms, case_sensitive)
        else:
            return self._filter_keywords_sequential(
                urls, terms, case_sensitive, search_content
            )

    def _filter_keywords_sequential(
        self,
        urls: List[str],
        terms: List[str],
        case_sensitive: bool,
        search_content: bool,
    ) -> List[Dict]:
        """Sequential keyword filtering with optional content search"""
        self.logger.info(
            f"é–‹å§‹é—œéµè©éæ¿¾: {len(terms)} å€‹é—œéµè©, æœå°‹å…§å®¹: {search_content}"
        )

        matching_articles = []
        total = len(urls)

        for i, url in enumerate(urls):
            try:
                html, from_wayback = self.fetch_html_content(url)
                if not html:
                    continue

                title = self.extract_title_from_html(html)
                title_matches = self.check_cjkv_keywords(title, terms, case_sensitive)

                if title_matches:
                    matching_articles.append(
                        {
                            "url": url,
                            "should_archive": True,
                            "title": title,
                            "matched_keywords": title_matches,
                            "from_wayback": from_wayback,
                            "title_search_only": True,
                        }
                    )
                    if i % 20 == 0:
                        self.logger.info(
                            f"é€²åº¦: {i}/{total}, æ‰¾åˆ°: {len(matching_articles)} ç¯‡åŒ¹é…"
                        )
                    continue

                if search_content:
                    content_matches = self.check_cjkv_keywords(
                        html, terms, case_sensitive
                    )
                    if content_matches:
                        all_matches = list(set(title_matches + content_matches))
                        matching_articles.append(
                            {
                                "url": url,
                                "should_archive": True,
                                "title": title,
                                "matched_keywords": all_matches,
                                "from_wayback": from_wayback,
                                "title_search_only": False,
                            }
                        )

            except Exception as e:
                self.logger.debug(f"é—œéµè©éæ¿¾å¤±æ•—: {url[:50]} - {str(e)}")
                continue

        percentage = (len(matching_articles) / total * 100) if total > 0 else 0
        self.logger.info(
            f"é—œéµè©éæ¿¾å®Œæˆ: {len(matching_articles)}/{total} ç¯‡åŒ¹é… ({percentage:.1f}%)"
        )
        return matching_articles

    def _filter_keywords_parallel(
        self, urls: List[str], terms: List[str], case_sensitive: bool
    ) -> List[Dict]:
        """Parallel keyword filtering (title-only)"""
        parallel_workers = self.config.get("keywords", {}).get("parallel_workers", 2)

        self.logger.info(
            f"é–‹å§‹é—œéµè©éæ¿¾ (ä¸¦è¡Œ {parallel_workers} workers): {len(terms)} å€‹é—œéµè©"
        )

        def process_url(url: str) -> Optional[Dict]:
            try:
                html, from_wayback = self.fetch_html_content(url)
                if not html:
                    return None

                title = self.extract_title_from_html(html)
                title_matches = self.check_cjkv_keywords(title, terms, case_sensitive)

                if title_matches:
                    return {
                        "url": url,
                        "should_archive": True,
                        "title": title,
                        "matched_keywords": title_matches,
                        "from_wayback": from_wayback,
                        "title_search_only": True,
                    }
            except Exception as e:
                self.logger.debug(f"Parallel filter failed: {url[:50]} - {str(e)}")
            return None

        matching_articles = []
        total = len(urls)

        with ThreadPoolExecutor(max_workers=parallel_workers) as executor:
            futures = {executor.submit(process_url, url): url for url in urls}
            completed = 0

            for future in as_completed(futures):
                completed += 1
                result = future.result()
                if result:
                    matching_articles.append(result)

                if completed % 20 == 0:
                    self.logger.info(
                        f"é€²åº¦: {completed}/{total}, æ‰¾åˆ°: {len(matching_articles)} ç¯‡åŒ¹é…"
                    )

        percentage = (len(matching_articles) / total * 100) if total > 0 else 0
        self.logger.info(
            f"é—œéµè©éæ¿¾å®Œæˆ: {len(matching_articles)}/{total} ç¯‡åŒ¹é… ({percentage:.1f}%)"
        )
        return matching_articles

    def record_keyword_result(
        self, article_data: Dict, archive_date: str, status: str = "success"
    ):
        """Record keyword matching result to database"""
        self.cursor.execute(
            """
            INSERT OR REPLACE INTO archive_records
            (article_url, wayback_url, archive_date, status, http_status, error_message,
             matched_keywords, checked_wayback, title_search_only, article_title, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        """,
            (
                article_data.get("url"),
                article_data.get("wayback_url"),
                archive_date,
                status,
                200 if status == "success" else None,
                None if status == "success" else status,
                ",".join(article_data.get("matched_keywords", [])),
                True,
                article_data.get("title_search_only", False),
                article_data.get("title"),
            ),
        )
        self.conn.commit()

    def check_url_exists(self, url: str) -> bool:
        """æª¢æŸ¥ URL æ˜¯å¦å­˜åœ¨"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            response = self._make_request(
                "HEAD", url, timeout=10, allow_redirects=True, headers=headers
            )
            return response.status_code == 200
        except Exception as e:
            self.logger.debug(f"æª¢æŸ¥ URL å¤±æ•—: {url} - {str(e)}")
            return False

    def check_urls_exist_in_db(self, urls: List[str]) -> Set[str]:
        """
        æ‰¹é‡æª¢æŸ¥ URL æ˜¯å¦å·²å­˜åœ¨æ–¼æ•¸æ“šåº«ä¸­ï¼ˆæ€§èƒ½å„ªåŒ–ï¼‰

        Args:
            urls: URL åˆ—è¡¨

        Returns:
            å·²å­˜åœ¨æ–¼æ•¸æ“šåº«ä¸­çš„ URL é›†åˆ
        """
        if not urls:
            return set()

        # Use batch query with IN clause instead of N individual queries
        placeholders = ",".join("?" * len(urls))
        query = f"SELECT article_url FROM archive_records WHERE article_url IN ({placeholders})"

        self.cursor.execute(query, urls)
        existing_urls = {row[0] for row in self.cursor.fetchall()}

        self.logger.debug(
            f"æ‰¹é‡æŸ¥è©¢: {len(urls)} å€‹ URL, {len(existing_urls)} å€‹å·²å­˜åœ¨"
        )
        return existing_urls

    def archive_to_wayback(self, url: str, retry_count=0) -> Dict:
        """å­˜æª”å–®å€‹ URL åˆ° Wayback Machine

        Note: Rate limiting is now handled by _make_request() wrapper
        """
        with self.stats_lock:
            self.stats["total_attempted"] += 1

        wayback_target = self.WAYBACK_SAVE_URL.format(url=url)
        config = self.config["archiving"]

        # Set User-Agent header for Wayback requests
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        try:
            response = self._make_request(
                "POST", wayback_target, timeout=config["timeout"], headers=headers
            )

            if response.status_code == 200:
                if "Content-Location" in response.headers:
                    wayback_url = (
                        f"https://web.archive.org{response.headers['Content-Location']}"
                    )
                    self.logger.info(f"âœ… å­˜æª”æˆåŠŸ: {url}")
                    self.logger.info(f"   Wayback: {wayback_url}")
                    with self.stats_lock:
                        self.stats["successful"] += 1
                    return {
                        "status": "success",
                        "wayback_url": wayback_url,
                        "http_status": response.status_code,
                        "error": None,
                    }
                else:
                    wayback_check = f"https://web.archive.org/web/2/{url}"
                    check_resp = self._make_request(
                        "GET", wayback_check, timeout=config["timeout"], headers=headers
                    )
                    if check_resp.status_code == 200:
                        self.logger.info(f"âš¡ å·²æœ‰å­˜æª”: {url}")
                        self.logger.info(f"   Wayback: {wayback_check}")
                        with self.stats_lock:
                            self.stats["already_archived"] += 1
                        return {
                            "status": "exists",
                            "wayback_url": wayback_check,
                            "http_status": response.status_code,
                            "error": None,
                        }
                    else:
                        self.logger.warning(f"âš ï¸  å­˜æª”ç‹€æ…‹ä¸æ˜: {url}")
                        with self.stats_lock:
                            self.stats["failed"] += 1
                        return {
                            "status": "unknown",
                            "wayback_url": None,
                            "http_status": response.status_code,
                            "error": "Save returned 200 but no Content-Location",
                        }

            elif response.status_code == 403:
                self.logger.warning(f"â³ Rate limited: {url}")
                with self.stats_lock:
                    self.stats["rate_limited"] += 1
                return {
                    "status": "rate_limited",
                    "wayback_url": None,
                    "http_status": response.status_code,
                    "error": "Rate limited",
                }

            else:
                self.logger.error(f"âŒ å¤±æ•— ({response.status_code}): {url}")
                with self.stats_lock:
                    self.stats["failed"] += 1
                return {
                    "status": "failed",
                    "wayback_url": None,
                    "http_status": response.status_code,
                    "error": f"HTTP {response.status_code}",
                }

        except requests.exceptions.Timeout:
            if retry_count < config["max_retries"]:
                self.logger.warning(
                    f"â±ï¸  è¶…æ™‚ï¼Œé‡è©¦ {retry_count + 1}/{config['max_retries']}: {url}"
                )
                time.sleep(config["retry_delay"])
                return self.archive_to_wayback(url, retry_count + 1)
            else:
                self.logger.error(f"â±ï¸  è¶…æ™‚ (é‡è©¦è€—ç›¡): {url}")
                with self.stats_lock:
                    self.stats["failed"] += 1
                return {
                    "status": "timeout",
                    "wayback_url": None,
                    "http_status": None,
                    "error": "Timeout after retries",
                }

        except Exception as e:
            self.logger.error(f"ğŸ’¥ ä¾‹å¤–éŒ¯èª¤: {url} - {str(e)}")
            with self.stats_lock:
                self.stats["failed"] += 1
            return {
                "status": "error",
                "wayback_url": None,
                "http_status": None,
                "error": str(e),
            }

    def record_attempt(self, article_url: str, result: Dict, archive_date: str):
        """è¨˜éŒ„å­˜æª”å˜—è©¦åˆ°æ•¸æ“šåº«"""
        self.cursor.execute(
            """
            INSERT OR REPLACE INTO archive_records 
            (article_url, wayback_url, archive_date, status, http_status, error_message, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
        """,
            (
                article_url,
                result.get("wayback_url"),
                archive_date,
                result["status"],
                result.get("http_status"),
                result.get("error"),
            ),
        )
        self.conn.commit()

    def _archive_url_worker(
        self, url: str, date_str: str, lock: threading.Lock
    ) -> Tuple[str, Dict]:
        """Worker function for parallel archiving"""
        conn = sqlite3.connect(self.config["database"]["path"])
        cursor = conn.cursor()

        result = self.archive_to_wayback(url)

        with lock:
            try:
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO archive_records 
                    (article_url, wayback_url, archive_date, status, http_status, error_message, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
                """,
                    (
                        url,
                        result.get("wayback_url"),
                        date_str,
                        result["status"],
                        result.get("http_status"),
                        result.get("error"),
                    ),
                )
                conn.commit()
            except Exception as e:
                self.logger.debug(f"DB error: {e}")
            finally:
                conn.close()

        return url, result

    def _log_start_banner(self, target_date: datetime, title_mode: str):
        """Log start banner for archiving operations"""
        date_str = target_date.strftime("%Y%m%d")
        self.logger.info("=" * 60)
        self.logger.info(
            f"é–‹å§‹è™•ç† ({title_mode}): {date_str} ({target_date.strftime('%Y-%m-%d %A')})"
        )
        self.logger.info("=" * 60)

    def _get_urls_to_process(
        self, date_str: str, archive_mode: str = "all"
    ) -> List[Dict]:
        """
        Get URLs to process with filtering applied

        Args:
            date_str: Date string in YYYYMMDD format
            archive_mode: "all" or "keywords"

        Returns:
            List of article dictionaries with at least 'url' key
        """
        from datetime import datetime

        target_date = datetime.strptime(date_str, "%Y%m%d")
        article_urls = self.generate_article_urls(target_date)

        self.logger.info(
            f"ç”Ÿæˆ {len(article_urls)} å€‹ URLï¼Œé–‹å§‹{'é—œéµè©' if archive_mode == 'keywords' else ''}éæ¿¾..."
        )

        if not article_urls:
            self.logger.warning("æ²’æœ‰ç”Ÿæˆä»»ä½• URL")
            return []

        existing_urls = self.check_urls_exist_in_db(article_urls)

        if archive_mode == "keywords":
            keywords_config = self.config.get("keywords", {})
            search_content = keywords_config.get("search_content", False)
            parallel_config = self.config.get("parallel", {})
            use_parallel = parallel_config.get("enabled", True) and not search_content

            matching_articles = self.filter_urls_by_keywords(
                article_urls, parallel=use_parallel
            )
            articles_to_process = [
                a for a in matching_articles if a["url"] not in existing_urls
            ]

            if matching_articles:
                filtered_count = len(article_urls) - len(matching_articles)
                self.logger.info(
                    f"å¾…è™•ç†: {len(articles_to_process)} ç¯‡åŒ¹é…æ–‡ç«  ({len(existing_urls)} å€‹å·²å­˜åœ¨)"
                )
            else:
                self.logger.warning("æ²’æœ‰æ‰¾åˆ°åŒ¹é…çš„é—œéµè©æ–‡ç« ")
                articles_to_process = []
        else:
            articles_to_process = [
                {"url": url} for url in article_urls if url not in existing_urls
            ]
            self.logger.info(
                f"å¾…è™•ç†: {len(articles_to_process)} å€‹æ–° URL ({len(existing_urls)} å€‹å·²å­˜åœ¨)"
            )

            if self.config["archiving"]["verify_first"]:
                articles_to_process = [
                    {"url": url}
                    for url in articles_to_process
                    if self.check_url_exists(url["url"])
                ]

        return articles_to_process

    def _record_daily_progress(
        self,
        date_str: str,
        found: int,
        archived: int,
        failed: int,
        not_found: int = 0,
        filtered: int = 0,
        execution_time: float = 0,
    ):
        """Record daily progress to database"""
        keywords_config = self.config.get("keywords", {})
        is_keyword_mode = keywords_config.get("enabled", False)

        if is_keyword_mode:
            self.cursor.execute(
                """
                INSERT OR REPLACE INTO daily_progress
                (date, articles_found, articles_archived, articles_failed, articles_not_found, 
                 execution_time, completed_at, keywords_filtered)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, ?)
            """,
                (
                    date_str,
                    found,
                    archived,
                    failed,
                    not_found,
                    execution_time,
                    filtered,
                ),
            )
        else:
            self.cursor.execute(
                """
                INSERT OR REPLACE INTO daily_progress 
                (date, articles_found, articles_archived, articles_failed, 
                 articles_not_found, execution_time, completed_at)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """,
                (date_str, found, archived, failed, not_found, execution_time),
            )
        self.conn.commit()

    def _archive_with_strategy(
        self, target_date: datetime, mode: str = "all", parallel: bool = False
    ) -> Dict:
        """Core archiving logic with configurable strategy"""
        date_str = target_date.strftime("%Y%m%d")
        title_mode = f"{'é—œéµè©' if mode == 'keywords' else ''}éæ¿¾{' (ä¸¦è¡Œ)' if parallel else ''}"

        self._log_start_banner(target_date, title_mode)

        if mode == "keywords":
            keywords = self.config.get("keywords", {}).get("terms", [])
            self.logger.info(f"é—œéµè©: {', '.join(keywords)}")

        start_time = time.time()
        articles_to_process = self._get_urls_to_process(date_str, archive_mode=mode)

        if not articles_to_process:
            filtered = 100  # Placeholder
            return {
                "date": date_str,
                "found": 0,
                "archived": 0,
                "failed": 0,
                "not_found": 0,
                "filtered": filtered if mode == "keywords" else 0,
                "time": time.time() - start_time,
            }

        found = archived = failed = 0
        not_found = 0 if mode == "all" else 0

        if parallel and mode == "all":
            found, archived, failed = self._archive_parallel(
                articles_to_process, date_str
            )
        else:
            found, archived, failed = self._archive_sequential(
                articles_to_process, date_str, mode
            )

        execution_time = time.time() - start_time
        filtered = (
            len(self.generate_article_urls(target_date)) - len(articles_to_process)
            if mode == "keywords"
            else 0
        )

        self._record_daily_progress(
            date_str, found, archived, failed, not_found, filtered, execution_time
        )

        self.logger.info("=" * 60)
        self.logger.info(f"å®Œæˆ: {date_str}")
        if mode == "keywords":
            self.logger.info(
                f"  æ‰¾åˆ°: {found} | æˆåŠŸ: {archived} | å¤±æ•—: {failed} | éæ¿¾: {filtered}"
            )
        else:
            self.logger.info(
                f"  æ‰¾åˆ°: {found} | æˆåŠŸ: {archived} | å¤±æ•—: {failed} | ä¸å­˜åœ¨: {not_found}"
            )
        self.logger.info(f"  æ™‚é–“: {execution_time:.1f} ç§’")
        self.logger.info("=" * 60)

        return {
            "date": date_str,
            "found": found,
            "archived": archived,
            "failed": failed,
            "not_found": not_found,
            "filtered": filtered if mode == "keywords" else 0,
            "time": execution_time,
        }

    def _archive_sequential(
        self, articles: List[Dict], date_str: str, mode: str
    ) -> Tuple[int, int, int]:
        """Sequential archiving of articles"""
        found = archived = failed = 0
        total = len(articles)

        for i, article in enumerate(articles):
            url = article["url"]
            found += 1

            result = self.archive_to_wayback(url)

            if mode == "keywords":
                article["wayback_url"] = result.get("wayback_url")
                self.record_keyword_result(article, date_str, result["status"])
                if result["status"] in ["success", "exists"]:
                    archived += 1
                    self.logger.info(
                        f"âœ… {', '.join(article.get('matched_keywords', []))}: {url[:60]}..."
                    )
                else:
                    failed += 1
            else:
                self.record_attempt(url, result, date_str)
                if result["status"] in ["success", "exists"]:
                    archived += 1
                else:
                    failed += 1

            if found >= self.config["daily_limit"]:
                self.logger.warning(f"é”åˆ°æ¯æ—¥é™åˆ¶: {self.config['daily_limit']}")
                break

            if i % 10 == 0 or i == total - 1:
                self.logger.info(f"é€²åº¦: {i + 1}/{total} ç¯‡å·²è™•ç†...")

        return found, archived, failed

    def _archive_parallel(
        self, articles: List[Dict], date_str: str
    ) -> Tuple[int, int, int]:
        """Parallel archiving of articles"""
        max_workers = self.config.get("parallel", {}).get("max_workers", 3)
        rate_delay = self.config.get("parallel", {}).get("rate_limit_delay", 1.0)

        found = archived = failed = 0
        lock = threading.Lock()

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    self._archive_url_worker, article["url"], date_str, lock
                ): article
                for article in articles
            }

            completed = 0
            total = len(futures)

            for future in as_completed(futures):
                completed += 1
                url, result = future.result()

                with self.stats_lock:
                    found += 1
                    if result["status"] in ["success", "exists"]:
                        archived += 1
                    else:
                        failed += 1

                if completed % 10 == 0 or completed == total:
                    progress_pct = (completed / total * 100) if total > 0 else 0
                    self.logger.info(f"é€²åº¦: {completed}/{total} ({progress_pct:.0f}%)")

                time.sleep(rate_delay / max_workers)

                if found >= self.config["daily_limit"]:
                    self.logger.warning(f"é”åˆ°æ¯æ—¥é™åˆ¶: {self.config['daily_limit']}")
                    for f in list(futures.keys()):
                        f.cancel()
                    break

        return found, archived, failed

    # Old deprecated methods removed - use archive_date() with appropriate config

    def archive_date(self, target_date: datetime):
        """å­˜æª”æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰ HK-GA æ–‡ç« """
        keywords_config = self.config.get("keywords", {})
        parallel_config = self.config.get("parallel", {})

        if keywords_config.get("enabled", False):
            search_content = keywords_config.get("search_content", False)
            use_parallel = parallel_config.get("enabled", True) and not search_content
            return self._archive_with_strategy(
                target_date, mode="keywords", parallel=use_parallel
            )

        use_parallel = parallel_config.get("enabled", True)
        return self._archive_with_strategy(
            target_date, mode="all", parallel=use_parallel
        )

    def archive_date_range(self, start_date: datetime, end_date: datetime):
        """å­˜æª”æŒ‡å®šæ—¥æœŸç¯„åœå…§çš„æ‰€æœ‰æ–‡ç« """
        self.logger.info(f"{'=' * 80}")
        self.logger.info(
            f"æ‰¹æ¬¡å­˜æª”: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"
        )
        self.logger.info(f"{'=' * 80}")

        total_days = (end_date - start_date).days + 1
        overall = {"days": 0, "found": 0, "archived": 0, "failed": 0}

        for day in range(total_days):
            process_date = start_date + timedelta(days=day)

            self.cursor.execute(
                "SELECT * FROM daily_progress WHERE date = ?",
                (process_date.strftime("%Y%m%d"),),
            )
            if self.cursor.fetchone():
                self.logger.info(f"è·³éå·²å®Œæˆ: {process_date.strftime('%Y-%m-%d')}")
                continue

            try:
                daily_stats = self.archive_date(process_date)
                overall["days"] += 1
                overall["found"] += daily_stats["found"]
                overall["archived"] += daily_stats["archived"]
                overall["failed"] += daily_stats["failed"]

                progress = (day + 1) / total_days * 100
                self.logger.info(
                    f"æ•´é«”é€²åº¦: {progress:.1f}% ({day + 1}/{total_days} å¤©)"
                )

                if day < total_days - 1:
                    self.logger.info("ç­‰å¾… 60 ç§’å¾Œç¹¼çºŒ...")
                    time.sleep(60)

            except Exception as e:
                self.logger.error(f"è™•ç† {process_date} æ™‚éŒ¯èª¤: {str(e)}")

        return overall

    def calculate_article_priority(
        self, title: str, keywords_matched: List[str]
    ) -> str:
        """Calculate priority based on keywords in title"""
        high_priority_keywords = ["é»æ™ºè‹±", "åœ‹å®‰è™•", "åœ‹å®‰æ³•", "23æ¢"]
        medium_priority_keywords = ["é¦™æ¸¯", "æ”¿æ²»", "ä¸­åœ‹", "å°ç£", "é¸èˆ‰", "ç¤ºå¨"]

        if not title:
            return "Low"

        title_lower = title.lower()
        title_kw_lower = (
            [kw.lower() for kw in keywords_matched] if keywords_matched else []
        )
        combined_text = title_lower + " ".join(title_kw_lower)

        for kw in high_priority_keywords:
            if kw.lower() in combined_text:
                return "High"

        count = 0
        for kw in medium_priority_keywords:
            if kw.lower() in combined_text:
                count += 1

        if count >= 2:
            return "High"
        elif count == 1:
            return "Medium"
        else:
            return "Low"

    def check_wayback_status_batch(
        self, urls: List[str], timeout: int = 15
    ) -> List[Tuple[str, str, Optional[str]]]:
        """Check Wayback status for multiple URLs"""
        results = []

        for url in urls:
            self.rate_limiter.acquire()
            exists, wayback_url = self.check_wayback_exists(url, timeout)
            if exists:
                results.append((url, "Already Archived", wayback_url))
            else:
                results.append((url, "Not Archived", None))

        return results

    def generate_article_csv(
        self,
        start_date: datetime,
        end_date: datetime,
        output_file: Optional[str] = None,
        verify_urls: bool = True,
        check_wayback: bool = True,
    ) -> Dict[str, any]:
        """Generate CSV for crowdsourced archiving

        Args:
            start_date: Start date for range
            end_date: End date for range
            output_file: Output CSV path (auto-generated if None)
            verify_urls: Verify URLs exist before including
            check_wayback: Check Wayback status for each URL

        Returns:
            Dict with statistics about generated CSV
        """
        import csv

        if not output_file:
            output_file = f"articles_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.csv"

        self.logger.info("=" * 80)
        self.logger.info(f"Generating CSV for crowdsourced archiving")
        self.logger.info(
            f"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"
        )
        self.logger.info(f"Output file: {output_file}")
        self.logger.info(f"Verify URLs: {verify_urls}")
        self.logger.info(f"Check Wayback: {check_wayback}")
        self.logger.info("=" * 80)

        start_time = time.time()

        keywords_config = self.config.get("keywords", {})
        keyword_terms = keywords_config.get("terms", [])

        total_days_processed = 0
        total_articles_found = 0
        total_need_archiving = 0
        total_already_archived = 0
        total_nonexistent = 0

        with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(
                [
                    "Date",
                    "Title",
                    "URL",
                    "Wayback Status",
                    "Priority",
                    "Keywords",
                    "Notes",
                ]
            )

            total_days = (end_date - start_date).days + 1

            for day in range(total_days):
                process_date = start_date + timedelta(days=day)
                date_str = process_date.strftime("%Y%m%d")
                date_display = process_date.strftime("%Y-%m-%d")

                self.logger.info(f"Processing date: {date_display}")

                article_urls = self.generate_article_urls(process_date)

                if verify_urls:
                    self.logger.info(f"  Verifying {len(article_urls)} URLs...")
                    existing_urls = []
                    for url in article_urls:
                        if self.check_url_exists(url):
                            existing_urls.append(url)

                    self.logger.info(f"  Found {len(existing_urls)} valid URLs")
                    urls_to_process = existing_urls
                else:
                    urls_to_process = article_urls

                if not urls_to_process:
                    self.logger.warning(f"  No URLs to process for {date_display}")
                    continue

                if check_wayback:
                    self.logger.info(
                        f"  Checking Wayback status for {len(urls_to_process)} URLs..."
                    )
                    wayback_results = self.check_wayback_status_batch(urls_to_process)
                else:
                    wayback_results = [
                        (url, "Unknown", None) for url in urls_to_process
                    ]

                self.logger.info(f"  Extracting titles and generating CSV rows...")
                processed_for_date = 0
                archived_for_date = 0
                need_archiving_for_date = 0

                for i, url in enumerate(urls_to_process):
                    try:
                        wayback_status = wayback_results[i][1]
                        wayback_url = wayback_results[i][2]

                        if wayback_status == "Already Archived":
                            archived_for_date += 1
                            writer.writerow(
                                [
                                    date_display,
                                    "",
                                    url,
                                    wayback_status,
                                    "",
                                    "",
                                    "Already in Wayback",
                                ]
                            )
                            continue

                        html, _ = self.fetch_html_content(url)
                        if not html:
                            writer.writerow(
                                [
                                    date_display,
                                    "",
                                    url,
                                    wayback_status,
                                    "",
                                    "",
                                    "Failed to fetch content",
                                ]
                            )
                            continue

                        title = self.extract_title_from_html(html)
                        if not title:
                            title = self.extract_title_with_newspaper4k(url)

                        matched_keywords = (
                            self.check_cjkv_keywords(title, keyword_terms)
                            if title
                            else []
                        )
                        priority = self.calculate_article_priority(
                            title, matched_keywords
                        )
                        keywords_str = (
                            ",".join(matched_keywords) if matched_keywords else ""
                        )

                        writer.writerow(
                            [
                                date_display,
                                title,
                                url,
                                wayback_status,
                                priority,
                                keywords_str,
                                "",
                            ]
                        )

                        processed_for_date += 1
                        if wayback_status == "Not Archived":
                            need_archiving_for_date += 1

                        if (i + 1) % 10 == 0:
                            self.logger.info(
                                f"    Processed {i + 1}/{len(urls_to_process)} URLs"
                            )

                    except Exception as e:
                        self.logger.debug(f"Error processing {url}: {str(e)}")
                        writer.writerow(
                            [
                                date_display,
                                "",
                                url,
                                "Unknown",
                                "",
                                "",
                                f"Error: {str(e)}",
                            ]
                        )
                        continue

                total_days_processed += 1
                total_articles_found += processed_for_date
                total_already_archived += archived_for_date
                total_need_archiving += need_archiving_for_date
                total_nonexistent += (
                    len(article_urls) - len(urls_to_process) if verify_urls else 0
                )

                self.logger.info(
                    f"  Date {date_display}: "
                    f"{processed_for_date} processed, "
                    f"{need_archiving_for_date} need archiving, "
                    f"{archived_for_date} already archived"
                )

                if day < total_days - 1:
                    rate_delay = self.config["archiving"]["rate_limit_delay"]
                    self.logger.info(f"  Waiting {rate_delay}s before next date...")
                    time.sleep(rate_delay)

        execution_time = time.time() - start_time

        self.logger.info("=" * 80)
        self.logger.info("CSV Generation Complete!")
        self.logger.info(f"Output file: {output_file}")
        self.logger.info(f"Time taken: {execution_time:.1f} seconds")
        self.logger.info("=" * 80)
        self.logger.info(f"Days processed: {total_days_processed}")
        self.logger.info(f"Total articles found: {total_articles_found}")
        self.logger.info(f"Need archiving: {total_need_archiving}")
        self.logger.info(f"Already archived: {total_already_archived}")
        self.logger.info(f"Non-existent URLs: {total_nonexistent}")
        self.logger.info("=" * 80)

        return {
            "output_file": output_file,
            "days_processed": total_days_processed,
            "total_articles_found": total_articles_found,
            "need_archiving": total_need_archiving,
            "already_archived": total_already_archived,
            "nonexistent_urls": total_nonexistent,
            "execution_time": execution_time,
        }

    def generate_report(self):
        """ç”Ÿæˆå­˜æª”å ±å‘Š"""
        self.cursor.execute("SELECT COUNT(*) FROM archive_records")
        total = self.cursor.fetchone()[0]

        self.cursor.execute(
            "SELECT COUNT(*) FROM archive_records WHERE status = 'success'"
        )
        success = self.cursor.fetchone()[0]

        self.cursor.execute("SELECT COUNT(*) FROM daily_progress")
        days = self.cursor.fetchone()[0]

        # é¿å…é™¤ä»¥ 0 éŒ¯èª¤
        success_rate = (success / total * 100) if total > 0 else 0

        report = f"""
{"=" * 60}
æ˜å ±æ¸¯è HK-GA å­˜æª”å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
{"=" * 60}
        
ğŸ“Š ç¸½è¨ˆ:
â€¢ è™•ç†å¤©æ•¸: {days} å¤©
â€¢ ç¸½å˜—è©¦: {total} ç¯‡æ–‡ç« 
â€¢ æˆåŠŸ: {success} ç¯‡
â€¢ æˆåŠŸç‡: {success_rate:.1f}%
        
ğŸ“ˆ çµ±è¨ˆ:
â€¢ å·²å˜—è©¦: {self.stats["total_attempted"]}
â€¢ æˆåŠŸ: {self.stats["successful"]}
â€¢ å·²å­˜åœ¨: {self.stats["already_archived"]}
â€¢ å¤±æ•—: {self.stats["failed"]}
â€¢ Rate limited: {self.stats["rate_limited"]}
        
ğŸ’¾ æ–‡ä»¶:
â€¢ æ•¸æ“šåº«: {self.config["database"]["path"]}
â€¢ æ—¥èªŒ: {self.config["logging"]["file"]}
{"=" * 60}
        """

        print(report)

        with open("output/archive_report.txt", "w", encoding="utf-8") as f:
            f.write(report)

        self.logger.info("å ±å‘Šå·²ç”Ÿæˆ: output/archive_report.txt")

    def close(self):
        """é—œé–‰æ•¸æ“šåº«é€£æ¥"""
        self.generate_report()
        self.conn.close()
        self.logger.info("æ•¸æ“šåº«é€£æ¥å·²é—œé–‰")


def parse_date(date_str: str) -> datetime:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    for fmt in ["%Y-%m-%d", "%Y%m%d"]:
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    raise ValueError(f"ç„¡æ³•è§£ææ—¥æœŸ: {date_str}")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="æ˜å ±åŠ æ‹¿å¤§æ¸¯è (HK-GA) Wayback Machine å­˜æª”å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å­˜æª”æŒ‡å®šæ—¥æœŸç¯„åœ (å¾é…ç½®æ–‡ä»¶è®€å–)
  python run_archiver.py

  # å­˜æª”å–®ä¸€æ—¥æœŸ
  python run_archiver.py --date 2025-01-12

  # å­˜æª”æ—¥æœŸç¯„åœ
  python run_archiver.py --start 2025-01-01 --end 2025-01-31

  # ä½¿ç”¨è‡ªå®šç¾©é…ç½®æ–‡ä»¶
  python run_archiver.py --config my_config.json

  # å¾ä»Šå¤©é–‹å§‹å›æº¯ N å¤©
  python run_archiver.py --backdays 30
        """,
    )

    parser.add_argument(
        "--config", default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾‘ (é»˜èª: config.json)"
    )
    parser.add_argument("--date", help="å­˜æª”æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)")
    parser.add_argument("--start", help="é–‹å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)")
    parser.add_argument("--end", help="çµæŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)")
    parser.add_argument("--backdays", type=int, help="å¾ä»Šå¤©é–‹å§‹å›æº¯ N å¤©")
    parser.add_argument("--report", action="store_true", help="åƒ…ç”Ÿæˆå ±å‘Šï¼Œä¸åŸ·è¡Œå­˜æª”")
    parser.add_argument("--daily-limit", type=int, help="æ¯æ—¥å­˜æª”é™åˆ¶ (è¦†è“‹é…ç½®æ–‡ä»¶)")
    parser.add_argument(
        "--newspaper", action="store_true", help="ä½¿ç”¨ newspaper3k ç™¼ç¾æ–‡ç«  URL"
    )
    parser.add_argument(
        "--keyword",
        action="append",
        dest="keywords",
        default=[],
        help="é—œéµè© (å¯å¤šæ¬¡ä½¿ç”¨ï¼Œå¦‚: --keyword é¦™æ¸¯ --keyword æ”¿æ²»)",
    )
    parser.add_argument(
        "--keywords",
        dest="keywords_comma",
        help="é—œéµè©åˆ—è¡¨ (é€—è™Ÿåˆ†éš”ï¼Œå¦‚: é¦™æ¸¯,æ”¿æ²»,ä¸­åœ‹)",
    )
    parser.add_argument(
        "--search-content",
        action="store_true",
        help="æœå°‹æ–‡ç« å…§å®¹ (é è¨­åªæœå°‹æ¨™é¡Œï¼Œè¼ƒæ…¢)",
    )
    parser.add_argument(
        "--case-sensitive", action="store_true", help="å€åˆ†å¤§å°å¯« (é è¨­ä¸å€åˆ†)"
    )
    parser.add_argument(
        "--enable-keywords",
        action="store_true",
        help="å•Ÿç”¨é—œéµè©éæ¿¾ (éœ€è¦é…åˆ --keyword æˆ– config.json)",
    )
    parser.add_argument(
        "--disable-keywords",
        action="store_true",
        help="ç¦ç”¨é—œéµè©éæ¿¾ (è¦†è“‹ config.json)",
    )
    parser.add_argument(
        "--generate-csv",
        action="store_true",
        help="Generate CSV for crowdsourced archiving (no actual archiving)",
    )
    parser.add_argument(
        "--csv-output",
        help="Output CSV file path for crowdsourced archiving",
    )
    parser.add_argument(
        "--csv-no-verify",
        action="store_true",
        help="Skip URL verification in CSV generation (faster)",
    )
    parser.add_argument(
        "--csv-no-wayback-check",
        action="store_true",
        help="Skip Wayback status check in CSV generation (faster)",
    )

    args = parser.parse_args()

    # å‰µå»ºå­˜æª”å™¨å¯¦ä¾‹
    archiver = MingPaoHKGAArchiver(args.config)

    # è™•ç† daily limit è¦†è“‹
    if args.daily_limit:
        archiver.config["daily_limit"] = args.daily_limit
        archiver.logger.info(f"æ¯æ—¥é™åˆ¶è¨­ç‚º: {args.daily_limit}")

    # å¦‚æœä½¿ç”¨ newspaper3k
    if args.newspaper:
        archiver.config["use_newspaper"] = True
        archiver.logger.info("å•Ÿç”¨ newspaper3k URL ç™¼ç¾æ¨¡å¼")

    # è™•ç†é—œéµè©åƒæ•¸
    if args.disable_keywords:
        archiver.config["keywords"]["enabled"] = False
        archiver.logger.info("å·²ç¦ç”¨é—œéµè©éæ¿¾")

    if args.enable_keywords or args.keywords or args.keywords_comma:
        archiver.config["keywords"]["enabled"] = True
        archiver.logger.info("å·²å•Ÿç”¨é—œéµè©éæ¿¾")

    all_keywords = []
    if args.keywords:
        all_keywords.extend(args.keywords)
    if args.keywords_comma:
        all_keywords.extend([k.strip() for k in args.keywords_comma.split(",")])

    if all_keywords:
        archiver.config["keywords"]["terms"] = all_keywords
        archiver.logger.info(f"é—œéµè©: {', '.join(all_keywords)}")

    if args.search_content:
        archiver.config["keywords"]["search_content"] = True
        archiver.logger.info("å•Ÿç”¨å…§å®¹æœå°‹ (æ¨™é¡Œ + æ­£æ–‡)")

    if args.case_sensitive:
        archiver.config["keywords"]["case_sensitive"] = True
        archiver.logger.info("å€åˆ†å¤§å°å¯«")

    # å¦‚æœåªéœ€è¦å ±å‘Š
    if args.report:
        archiver.generate_report()
        archiver.close()
        return

    # è™•ç† CSV ç”Ÿæˆ
    if args.generate_csv:
        verify_urls = not args.csv_no_verify
        check_wayback = not args.csv_no_wayback_check
        output_file = args.csv_output

        end_date = datetime.now()
        if args.backdays:
            start_date = end_date - timedelta(days=args.backdays - 1)
        elif args.start and args.end:
            start_date = parse_date(args.start)
            end_date = parse_date(args.end)
        elif args.date:
            start_date = parse_date(args.date)
            end_date = start_date
        else:
            date_config = archiver.config["date_range"]
            start_date = parse_date(date_config["start"])
            end_date = parse_date(date_config["end"])

        archiver.logger.info("=" * 80)
        archiver.logger.info("CSV crowdsourced archiving generation mode")
        archiver.logger.info(
            f"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"
        )
        archiver.logger.info(f"Verify URLs: {verify_urls}")
        archiver.logger.info(f"Check Wayback: {check_wayback}")
        archiver.logger.info("=" * 80)

        try:
            result = archiver.generate_article_csv(
                start_date=start_date,
                end_date=end_date,
                output_file=output_file,
                verify_urls=verify_urls,
                check_wayback=check_wayback,
            )
            archiver.close()
            return
        except Exception as e:
            print(f"CSV generation error: {str(e)}")
            archiver.close()
            return

    # è™•ç†æ—¥æœŸåƒæ•¸
    today = datetime.now()

    if args.date:
        # å­˜æª”å–®ä¸€æ—¥æœŸ
        target_date = parse_date(args.date)
        archiver.logger.info(f"å­˜æª”å–®ä¸€æ—¥æœŸ: {target_date.strftime('%Y-%m-%d')}")
        try:
            result = archiver.archive_date(target_date)
        except Exception as e:
            print(f"åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            return
    elif args.start and args.end:
        # å­˜æª”æ—¥æœŸç¯„åœ
        start_date = parse_date(args.start)
        end_date = parse_date(args.end)
        archiver.logger.info(
            f"å­˜æª”æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
        )
        try:
            result = archiver.archive_date_range(start_date, end_date)
        except Exception as e:
            print(f"åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            return
    elif args.backdays:
        # å›æº¯ N å¤©
        end_date = today
        start_date = today - timedelta(days=args.backdays - 1)
        archiver.logger.info(
            f"å›æº¯ {args.backdays} å¤©: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
        )
        try:
            result = archiver.archive_date_range(start_date, end_date)
        except Exception as e:
            print(f"åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            return
    else:
        # å¾é…ç½®æ–‡ä»¶è®€å–
        date_config = archiver.config["date_range"]
        start_date = parse_date(date_config["start"])
        end_date = parse_date(date_config["end"])
        try:
            result = archiver.archive_date_range(start_date, end_date)
        except Exception as e:
            print(f"åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
            return

    # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
    archiver.logger.info("\n" + "=" * 60)
    archiver.logger.info("æœ€çµ‚çµ±è¨ˆ")
    archiver.logger.info("=" * 60)
    for key, value in archiver.stats.items():
        archiver.logger.info(f"  {key}: {value}")

    # é—œé–‰
    archiver.close()
    archiver.logger.info("æ•¸æ“šåº«é€£æ¥å·²é—œé–‰")


if __name__ == "__main__":
    main()
